{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAxtKdplRy6m"
      },
      "outputs": [],
      "source": [
        "!pip -q install -U transformers accelerate bitsandbytes datasets pandas\n",
        "\n",
        "import torch, re, time\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "if device == \"cuda\":\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "    )\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "else:\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
        "\n",
        "model.eval()\n",
        "print(\"Loaded:\", MODEL_NAME)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "N = 200\n",
        "ds = load_dataset(\"openlifescienceai/MedQA-USMLE-4-options-hf\", split=\"train\")\n",
        "\n",
        "ds_small = ds.shuffle(seed=0).select(range(N))\n",
        "\n",
        "def clean_text(x):\n",
        "    if not isinstance(x, str): return x\n",
        "    x = x.replace(\"\\n\", \" \")\n",
        "    return \" \".join(x.split())\n",
        "\n",
        "letters = [\"A\",\"B\",\"C\",\"D\"]\n",
        "option_cols = [\"ending0\",\"ending1\",\"ending2\",\"ending3\"]\n",
        "\n",
        "rows = []\n",
        "for ex in ds_small:\n",
        "    if any(ex.get(k) is None for k in [\"sent1\",\"sent2\",\"ending0\",\"ending1\",\"ending2\",\"ending3\",\"label\"]):\n",
        "        continue\n",
        "    q = clean_text(ex[\"sent1\"]) + \" \" + clean_text(ex[\"sent2\"])\n",
        "    opts = [clean_text(ex[c]) for c in option_cols]\n",
        "    lab = int(ex[\"label\"])\n",
        "    if lab not in [0,1,2,3]:\n",
        "        continue\n",
        "    rows.append({\n",
        "        \"question\": q,\n",
        "        \"options\": opts,\n",
        "        \"correct_letter\": letters[lab],\n",
        "    })\n",
        "\n",
        "print(\"Usable rows:\", len(rows))\n"
      ],
      "metadata": {
        "id": "x6F24_SqR2a2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_mcq_prompt(question_text, options):\n",
        "    \"\"\"\n",
        "    Builds a clean MCQ prompt for Qwen-style instruction models.\n",
        "    \"\"\"\n",
        "    return (\n",
        "        \"You are a medical expert answering USMLE-style questions.\\n\\n\"\n",
        "        \"Question:\\n\"\n",
        "        f\"{question_text}\\n\\n\"\n",
        "        \"Options:\\n\"\n",
        "        f\"A. {options[0]}\\n\"\n",
        "        f\"B. {options[1]}\\n\"\n",
        "        f\"C. {options[2]}\\n\"\n",
        "        f\"D. {options[3]}\\n\\n\"\n",
        "        \"Give ONLY the single best option letter: A, B, C, or D.\\n\"\n",
        "        \"Answer:\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "4iM27JXOSsMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def seq_logprob(prompt, completion_text):\n",
        "    \"\"\"\n",
        "    Computes log P(completion_text | prompt) using teacher forcing.\n",
        "    Works even when ' A' / ' B' tokenize into multiple tokens.\n",
        "    \"\"\"\n",
        "    # tokenize prompt and completion separately\n",
        "    p_ids = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(device)\n",
        "    c_ids = tokenizer(completion_text, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(device)\n",
        "\n",
        "    # concatenate\n",
        "    input_ids = torch.cat([p_ids, c_ids], dim=1)\n",
        "\n",
        "    # forward pass\n",
        "    outputs = model(input_ids=input_ids)\n",
        "    logits = outputs.logits[:, :-1, :]\n",
        "    targets = input_ids[:, 1:]\n",
        "\n",
        "    # indices corresponding to completion tokens\n",
        "    start = p_ids.shape[1] - 1\n",
        "    end = start + c_ids.shape[1]\n",
        "\n",
        "    log_probs = torch.log_softmax(logits, dim=-1)\n",
        "    token_logprobs = log_probs[0, start:end, :].gather(\n",
        "        1, targets[0, start:end].unsqueeze(1)\n",
        "    ).squeeze(1)\n",
        "\n",
        "    return float(token_logprobs.sum().cpu())\n",
        "\n",
        "\n",
        "def option_probs_from_model(prompt):\n",
        "    \"\"\"\n",
        "    Returns P(A), P(B), P(C), P(D) as a numpy array.\n",
        "    \"\"\"\n",
        "    completions = [\" A\", \" B\", \" C\", \" D\"]\n",
        "    logps = torch.tensor(\n",
        "        [seq_logprob(prompt, c) for c in completions],\n",
        "        dtype=torch.float32\n",
        "    )\n",
        "    probs = torch.softmax(logps, dim=0).cpu().numpy()\n",
        "    return probs\n",
        "\n",
        "\n",
        "def predict_letter_from_probs(probs):\n",
        "    letters = [\"A\", \"B\", \"C\", \"D\"]\n",
        "    return letters[int(probs.argmax())]\n"
      ],
      "metadata": {
        "id": "8J5EuY5rS1HQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def paraphrase_question_with_qwen(question_text, max_new_tokens=96):\n",
        "    prompt = (\n",
        "        \"You are a medical doctor.\\n\\n\"\n",
        "        \"Rewrite the following clinical question in different words while keeping \"\n",
        "        \"ALL the medical meaning and clinical facts the same.\\n\"\n",
        "        \"Do NOT change the clinical facts. Do NOT add new information.\\n\"\n",
        "        \"Output ONLY the rewritten question, nothing else.\\n\\n\"\n",
        "        f\"Original question:\\n{question_text}\\n\\n\"\n",
        "        \"Rewritten question:\"\n",
        "    )\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    out_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=False,\n",
        "        num_beams=1,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    raw = tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    if raw.startswith(prompt):\n",
        "        return raw[len(prompt):].strip()\n",
        "\n",
        "    lines = [l.strip() for l in raw.split(\"\\n\") if l.strip()]\n",
        "    return lines[-1] if lines else raw.strip()\n",
        "\n",
        "records = []\n",
        "t0 = time.time()\n",
        "\n",
        "for i, r in enumerate(rows):\n",
        "    q = r[\"question\"]\n",
        "    opts = r[\"options\"]\n",
        "    correct = r[\"correct_letter\"]\n",
        "\n",
        "    base_prompt = build_mcq_prompt(q, opts)\n",
        "    base_probs = option_probs_from_model(base_prompt)\n",
        "    base_pred = predict_letter_from_probs(base_probs)\n",
        "\n",
        "    q_para = paraphrase_question_with_qwen(q)\n",
        "    para_prompt = build_mcq_prompt(q_para, opts)\n",
        "    para_probs = option_probs_from_model(para_prompt)\n",
        "    para_pred = predict_letter_from_probs(para_probs)\n",
        "\n",
        "    records.append({\n",
        "        \"qid\": i,\n",
        "        \"question_original\": q,\n",
        "        \"question_paraphrased\": q_para,\n",
        "        \"option_A\": opts[0], \"option_B\": opts[1], \"option_C\": opts[2], \"option_D\": opts[3],\n",
        "        \"correct_letter\": correct,\n",
        "        \"baseline_pred\": base_pred,\n",
        "        \"paraphrased_pred\": para_pred,\n",
        "        \"baseline_correct\": (base_pred == correct),\n",
        "        \"paraphrased_correct\": (para_pred == correct),\n",
        "        \"prediction_flip\": (base_pred != para_pred),\n",
        "    })\n",
        "\n",
        "    if (i+1) % 10 == 0:\n",
        "        print(f\"Paraphrase test: {i+1}/{len(rows)} done | {time.time()-t0:.1f}s\")\n",
        "\n",
        "para_df = pd.DataFrame(records)\n",
        "para_df.to_csv(\"qwen_paraphrase_results_200.csv\", index=False)\n",
        "print(\"Saved: qwen_paraphrase_results_200.csv\")\n",
        "para_df.head()\n"
      ],
      "metadata": {
        "id": "A76KQ2WNR407"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def get_influential_words_lite(question_text, options, max_words=5, max_new_tokens=24):\n",
        "    opt_block = (\n",
        "        f\"A. {options[0]}\\n\"\n",
        "        f\"B. {options[1]}\\n\"\n",
        "        f\"C. {options[2]}\\n\"\n",
        "        f\"D. {options[3]}\"\n",
        "    )\n",
        "    ask_prompt = (\n",
        "        \"You are a medical doctor.\\n\\n\"\n",
        "        f\"Question:\\n{question_text}\\n\\n\"\n",
        "        f\"Options:\\n{opt_block}\\n\\n\"\n",
        "        \"List the single most important words from the QUESTION that influenced your MCQ choice.\\n\"\n",
        "        \"Output ONLY a comma-separated list of individual words, e.g.\\n\"\n",
        "        \"fever,cough,chest,pain\\n\"\n",
        "        \"No sentences. No explanations.\\n\"\n",
        "        \"Words:\"\n",
        "    )\n",
        "    inputs = tokenizer(ask_prompt, return_tensors=\"pt\").to(device)\n",
        "    out_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=False,\n",
        "        num_beams=1,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    raw_text = tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    completion = raw_text[len(ask_prompt):].strip() if raw_text.startswith(ask_prompt) else raw_text.split(\"\\n\")[-1].strip()\n",
        "\n",
        "    cand_words = re.findall(r\"[a-z]+\", completion.lower())\n",
        "    question_vocab = set(re.findall(r\"[a-z]+\", question_text.lower()))\n",
        "\n",
        "    words = []\n",
        "    for w in cand_words:\n",
        "        if w in question_vocab and w not in words:\n",
        "            words.append(w)\n",
        "        if len(words) >= max_words:\n",
        "            break\n",
        "    return words\n",
        "\n",
        "def redact_words(text, words):\n",
        "    if not words:\n",
        "        return text\n",
        "    pattern = r\"\\b(\" + \"|\".join(re.escape(w) for w in words) + r\")\\b\"\n",
        "    red = re.sub(pattern, \"____\", text, flags=re.IGNORECASE)\n",
        "    return \" \".join(red.split())\n",
        "\n",
        "attrib_rows = []\n",
        "t0 = time.time()\n",
        "\n",
        "for i, r in enumerate(rows):\n",
        "    q = r[\"question\"]\n",
        "    opts = r[\"options\"]\n",
        "    correct = r[\"correct_letter\"]\n",
        "    correct_idx = letters.index(correct)\n",
        "\n",
        "    base_prompt = build_mcq_prompt(q, opts)\n",
        "    base_probs = option_probs_from_model(base_prompt)\n",
        "    base_pred = predict_letter_from_probs(base_probs)\n",
        "    base_p_correct = float(base_probs[correct_idx])\n",
        "\n",
        "    infl = get_influential_words_lite(q, opts, max_words=5)\n",
        "    if len(infl) == 0:\n",
        "        continue\n",
        "\n",
        "    q_red = redact_words(q, infl)\n",
        "    red_prompt = build_mcq_prompt(q_red, opts)\n",
        "    red_probs = option_probs_from_model(red_prompt)\n",
        "    red_pred = predict_letter_from_probs(red_probs)\n",
        "    red_p_correct = float(red_probs[correct_idx])\n",
        "\n",
        "    attrib_rows.append({\n",
        "        \"qid\": i,\n",
        "        \"question\": q,\n",
        "        \"question_redacted\": q_red,\n",
        "        \"options\": opts,\n",
        "        \"correct_letter\": correct,\n",
        "        \"influential_words\": infl,\n",
        "        \"num_inf\": len(infl),\n",
        "        \"baseline_probs\": base_probs.tolist(),\n",
        "        \"redacted_probs\": red_probs.tolist(),\n",
        "        \"baseline_p_correct\": base_p_correct,\n",
        "        \"redacted_p_correct\": red_p_correct,\n",
        "        \"delta_p_correct\": base_p_correct - red_p_correct,\n",
        "        \"baseline_pred\": base_pred,\n",
        "        \"redacted_pred\": red_pred,\n",
        "        \"pred_flip\": (base_pred != red_pred),\n",
        "    })\n",
        "\n",
        "    if (i+1) % 10 == 0:\n",
        "        print(f\"Attribution test: {i+1}/{len(rows)} done | {time.time()-t0:.1f}s\")\n",
        "\n",
        "attrib_df = pd.DataFrame(attrib_rows)\n",
        "attrib_df.to_csv(\"qwen_attribution_redaction_200.csv\", index=False)\n",
        "print(\"Saved: qwen_attribution_redaction_200.csv | rows:\", len(attrib_df))\n",
        "attrib_df.head()\n"
      ],
      "metadata": {
        "id": "4HYGfIT7R7Eh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}